openapi: 3.0.1
info:
  title: Ollama API
  description: API Spec for Ollama API. Please see https://github.com/jmorganca/ollama/blob/main/docs/api.md for more details.
  version: 0.1.36
servers:
  - url: http://localhost:11434/api
    description: Ollama server URL
paths:
  /version:
    get:
      summary: Returns the version of the Ollama server.
      description: This endpoint returns the version of the Ollama server.
      operationId: getVersion
      responses:
        '200':
          description: Successful operation.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/VersionResponse'
  /generate:
    post:
      tags:
        - Completions
      summary: Generate a response for a given prompt with a provided model.
      description: The final response object will include statistics and additional data from the request.
      operationId: generateCompletion
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateCompletionRequest'
      responses:
        '200':
          description: Successful operation.
          content:
            application/x-ndjson:
              schema:
                $ref: '#/components/schemas/GenerateCompletionResponse'
  /chat:
    post:
      tags:
        - Chat
      summary: Generate the next message in a chat with a provided model.
      description: 'This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.'
      operationId: generateChatCompletion
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateChatCompletionRequest'
      responses:
        '200':
          description: Successful operation.
          content:
            application/x-ndjson:
              schema:
                $ref: '#/components/schemas/GenerateChatCompletionResponse'
  /embeddings:
    post:
      tags:
        - Embeddings
      summary: Generate embeddings from a model.
      operationId: generateEmbedding
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateEmbeddingRequest'
      responses:
        '200':
          description: Successful operation.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GenerateEmbeddingResponse'
  /create:
    post:
      tags:
        - Models
      summary: Create a model from a Modelfile.
      description: 'It is recommended to set `modelfile` to the content of the Modelfile rather than just set `path`. This is a requirement for remote create. Remote model creation should also create any file blobs, fields such as `FROM` and `ADAPTER`, explicitly with the server using Create a Blob and the value to the path indicated in the response.'
      operationId: createModel
      requestBody:
        description: Create a new model from a Modelfile.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateModelRequest'
      responses:
        '200':
          description: Successful operation.
          content:
            application/x-ndjson:
              schema:
                $ref: '#/components/schemas/CreateModelResponse'
  /tags:
    get:
      tags:
        - Models
      summary: List models that are available locally.
      operationId: listModels
      responses:
        '200':
          description: Successful operation.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelsResponse'
  /ps:
    get:
      tags:
        - Models
      summary: List models that are running.
      operationId: listRunningModels
      responses:
        '200':
          description: Successful operation.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ProcessResponse'
  /show:
    post:
      tags:
        - Models
      summary: 'Show details about a model including modelfile, template, parameters, license, and system prompt.'
      operationId: showModelInfo
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ModelInfoRequest'
      responses:
        '200':
          description: Successful operation.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelInfo'
  /copy:
    post:
      tags:
        - Models
      summary: Creates a model with another name from an existing model.
      operationId: copyModel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CopyModelRequest'
      responses:
        '200':
          description: Successful operation.
  /delete:
    delete:
      tags:
        - Models
      summary: Delete a model and its data.
      operationId: deleteModel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/DeleteModelRequest'
      responses:
        '200':
          description: Successful operation.
  /pull:
    post:
      tags:
        - Models
      summary: Download a model from the ollama library.
      description: 'Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.'
      operationId: pullModel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/PullModelRequest'
      responses:
        '200':
          description: Successful operation.
          content:
            application/x-ndjson:
              schema:
                $ref: '#/components/schemas/PullModelResponse'
  /push:
    post:
      tags:
        - Models
      summary: Upload a model to a model library.
      description: Requires registering for ollama.ai and adding a public key first.
      operationId: pushModel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/PushModelRequest'
      responses:
        '200':
          description: Successful operation.
          content:
            application/x-ndjson:
              schema:
                $ref: '#/components/schemas/PushModelResponse'
  '/blobs/{digest}':
    head:
      tags:
        - Models
      summary: Ensures that the file blob used for a FROM or ADAPTER field exists on the server.
      description: This is checking your Ollama server and not Ollama.ai.
      operationId: checkBlob
      parameters:
        - name: digest
          in: path
          description: the SHA256 digest of the blob
          required: true
          schema:
            type: string
          example: sha256:c8edda1f17edd2f1b60253b773d837bda7b9d249a61245931a4d7c9a8d350250
      responses:
        '200':
          description: Blob exists on the server
        '404':
          description: Blob was not found
    post:
      tags:
        - Models
      summary: Create a blob from a file. Returns the server file path.
      operationId: createBlob
      parameters:
        - name: digest
          in: path
          description: the SHA256 digest of the blob
          required: true
          schema:
            type: string
          example: sha256:c8edda1f17edd2f1b60253b773d837bda7b9d249a61245931a4d7c9a8d350250
      requestBody:
        content:
          application/octet-stream:
            schema:
              type: string
              format: binary
      responses:
        '201':
          description: Blob was successfully created
components:
  schemas:
    GenerateCompletionRequest:
      required:
        - model
        - prompt
      type: object
      properties:
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: llama3.2
        prompt:
          type: string
          description: The prompt to generate a response.
          example: Why is the sky blue?
        suffix:
          type: string
          description: The text that comes after the inserted text.
        images:
          type: array
          items:
            type: string
            description: Base64-encoded image (for multimodal models such as llava)
            example: iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
          description: (optional) a list of Base64-encoded images to include in the message (for multimodal models such as llava)
        system:
          type: string
          description: The system prompt to (overrides what is defined in the Modelfile).
        template:
          type: string
          description: The full prompt or prompt template (overrides what is defined in the Modelfile).
        context:
          type: array
          items:
            type: integer
            format: int64
          description: 'The context parameter returned from a previous request to [generateCompletion], this can be used to keep a short conversational memory.'
        options:
          $ref: '#/components/schemas/RequestOptions'
        format:
          $ref: '#/components/schemas/ResponseFormat'
        raw:
          type: boolean
          description: "If `true` no formatting will be applied to the prompt and no context will be returned. \n\nYou may choose to use the `raw` parameter if you are specifying a full templated prompt in your request to the API, and are managing history yourself.\n"
        stream:
          type: boolean
          description: "If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.\n"
          default: true
        keep_alive:
          type: integer
          description: "How long (in minutes) to keep the model loaded in memory.\n\n- If set to a positive duration (e.g. 20), the model will stay loaded for the provided duration.\n- If set to a negative duration (e.g. -1), the model will stay loaded indefinitely.\n- If set to 0, the model will be unloaded immediately once finished.\n- If not set, the model will stay loaded for 5 minutes by default\n"
          nullable: true
      description: Request class for the generate endpoint.
    RequestOptions:
      type: object
      properties:
        num_keep:
          type: integer
          description: "Number of tokens to keep from the prompt.\n"
          nullable: true
        seed:
          type: integer
          description: "Sets the random number seed to use for generation. Setting this to a specific number will make the model \ngenerate the same text for the same prompt. (Default: 0)\n"
          nullable: true
        num_predict:
          type: integer
          description: "Maximum number of tokens to predict when generating text. \n(Default: 128, -1 = infinite generation, -2 = fill context)\n"
          nullable: true
        top_k:
          type: integer
          description: "Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, \nwhile a lower value (e.g. 10) will be more conservative. (Default: 40)\n"
          nullable: true
        top_p:
          type: number
          description: "Works together with top_k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value \n(e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\n"
          format: float
          nullable: true
        min_p:
          type: number
          description: "Alternative to the top_p, and aims to ensure a balance of quality and variety. min_p represents the minimum \nprobability for a token to be considered, relative to the probability of the most likely token. For \nexample, with min_p=0.05 and the most likely token having a probability of 0.9, logits with a value less \nthan 0.05*0.9=0.045 are filtered out. (Default: 0.0)\n"
          format: float
          nullable: true
        tfs_z:
          type: number
          description: "Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value \n(e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (default: 1)\n"
          format: float
          nullable: true
        typical_p:
          type: number
          description: "Typical p is used to reduce the impact of less probable tokens from the output. (default: 1)\n"
          format: float
          nullable: true
        repeat_last_n:
          type: integer
          description: "Sets how far back for the model to look back to prevent repetition. \n(Default: 64, 0 = disabled, -1 = num_ctx)\n"
          nullable: true
        temperature:
          type: number
          description: "The temperature of the model. Increasing the temperature will make the model answer more creatively. \n(Default: 0.8)\n"
          format: float
          nullable: true
        repeat_penalty:
          type: number
          description: "Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more \nstrongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)\n"
          format: float
          nullable: true
        presence_penalty:
          type: number
          description: "Positive values penalize new tokens based on whether they appear in the text so far, increasing the \nmodel's likelihood to talk about new topics. (Default: 0)\n"
          format: float
          nullable: true
        frequency_penalty:
          type: number
          description: "Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the \nmodel's likelihood to repeat the same line verbatim. (Default: 0)\n"
          format: float
          nullable: true
        mirostat:
          type: integer
          description: "Enable Mirostat sampling for controlling perplexity. \n(default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n"
          nullable: true
        mirostat_tau:
          type: number
          description: "Controls the balance between coherence and diversity of the output. A lower value will result in more \nfocused and coherent text. (Default: 5.0)\n"
          format: float
          nullable: true
        mirostat_eta:
          type: number
          description: "Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate \nwill result in slower adjustments, while a higher learning rate will make the algorithm more responsive. \n(Default: 0.1)\n"
          format: float
          nullable: true
        penalize_newline:
          type: boolean
          description: "Penalize newlines in the output. (Default: true)\n"
          nullable: true
        stop:
          type: array
          items:
            type: string
          description: "Sequences where the API will stop generating further tokens. The returned text will not contain the stop \nsequence.\n"
          nullable: true
        numa:
          type: boolean
          description: "Enable NUMA support. (Default: false)\n"
          nullable: true
        num_ctx:
          type: integer
          description: "Sets the size of the context window used to generate the next token. (Default: 2048)\n"
          nullable: true
        num_batch:
          type: integer
          description: "Sets the number of batches to use for generation. (Default: 512)\n"
          nullable: true
        num_gpu:
          type: integer
          description: "The number of layers to send to the GPU(s). \nOn macOS it defaults to 1 to enable metal support, 0 to disable.\n"
          nullable: true
        main_gpu:
          type: integer
          description: "The GPU to use for the main model. Default is 0.\n"
          nullable: true
        low_vram:
          type: boolean
          description: "Enable low VRAM mode. (Default: false)\n"
          nullable: true
        f16_kv:
          type: boolean
          description: "Enable f16 key/value. (Default: true)\n"
          nullable: true
        logits_all:
          type: boolean
          description: "Enable logits all. (Default: false)\n"
          nullable: true
        vocab_only:
          type: boolean
          description: "Enable vocab only. (Default: false)\n"
          nullable: true
        use_mmap:
          type: boolean
          description: "Enable mmap. (Default: false)\n"
          nullable: true
        use_mlock:
          type: boolean
          description: "Enable mlock. (Default: false)\n"
          nullable: true
        num_thread:
          type: integer
          description: "Sets the number of threads to use during computation. By default, Ollama will detect this for optimal \nperformance. It is recommended to set this value to the number of physical CPU cores your system has \n(as opposed to the logical number of cores).\n"
          nullable: true
      description: Additional model parameters listed in the documentation for the Modelfile such as `temperature`.
    ResponseFormat:
      enum:
        - json
      type: string
      description: "The format to return a response in. Currently the only accepted value is json.\n\nEnable JSON mode by setting the format parameter to json. This will structure the response as valid JSON.\n\nNote: it's important to instruct the model to use JSON in the prompt. Otherwise, the model may generate large amounts whitespace.\n"
    VersionResponse:
      type: object
      properties:
        version:
          type: string
          description: The version of the Ollama server.
      description: The response class for the version endpoint.
    GenerateCompletionResponse:
      type: object
      properties:
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: llama3.2
        created_at:
          type: string
          description: Date on which a model was created.
          format: date-time
          example: '2023-08-04T19:22:45.4991270+00:00'
        response:
          type: string
          description: The response for a given prompt with a provided model.
          example: The sky appears blue because of a phenomenon called Rayleigh scattering.
        done:
          type: boolean
          description: Whether the response has completed.
          example: true
        context:
          type: array
          items:
            type: integer
            format: int64
          description: "An encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory.\n"
          example:
            - 1
            - 2
            - 3
        total_duration:
          type: integer
          description: Time spent generating the response.
          format: int64
          example: 5589157167
        load_duration:
          type: integer
          description: Time spent in nanoseconds loading the model.
          format: int64
          example: 3013701500
        prompt_eval_count:
          type: integer
          description: Number of tokens in the prompt.
          example: 46
        prompt_eval_duration:
          type: integer
          description: Time spent in nanoseconds evaluating the prompt.
          format: int64
          example: 1160282000
        eval_count:
          type: integer
          description: Number of tokens the response.
          example: 113
        eval_duration:
          type: integer
          description: Time in nanoseconds spent generating the response.
          format: int64
          example: 1325948000
      description: The response class for the generate endpoint.
    GenerateChatCompletionRequest:
      required:
        - model
        - messages
      type: object
      properties:
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: llama3.2
        messages:
          type: array
          items:
            $ref: '#/components/schemas/Message'
          description: 'The messages of the chat, this can be used to keep a chat memory'
        format:
          $ref: '#/components/schemas/ResponseFormat'
        options:
          $ref: '#/components/schemas/RequestOptions'
        stream:
          type: boolean
          description: "If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.\n"
          default: true
        keep_alive:
          type: integer
          description: "How long (in minutes) to keep the model loaded in memory.\n\n- If set to a positive duration (e.g. 20), the model will stay loaded for the provided duration.\n- If set to a negative duration (e.g. -1), the model will stay loaded indefinitely.\n- If set to 0, the model will be unloaded immediately once finished.\n- If not set, the model will stay loaded for 5 minutes by default\n"
          nullable: true
        tools:
          type: array
          items:
            $ref: '#/components/schemas/Tool'
          description: A list of tools the model may call.
      description: Request class for the chat endpoint.
    GenerateChatCompletionResponse:
      required:
        - model
        - created_at
        - message
        - done
      type: object
      properties:
        message:
          $ref: '#/components/schemas/Message'
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: llama3.2
        created_at:
          type: string
          description: Date on which a model was created.
          format: date-time
          example: '2023-08-04T19:22:45.4991270+00:00'
        done:
          type: boolean
          description: Whether the response has completed.
          example: true
        done_reason:
          $ref: '#/components/schemas/DoneReason'
        total_duration:
          type: integer
          description: Time spent generating the response.
          format: int64
          example: 5589157167
        load_duration:
          type: integer
          description: Time spent in nanoseconds loading the model.
          format: int64
          example: 3013701500
        prompt_eval_count:
          type: integer
          description: Number of tokens in the prompt.
          example: 46
        prompt_eval_duration:
          type: integer
          description: Time spent in nanoseconds evaluating the prompt.
          format: int64
          example: 1160282000
        eval_count:
          type: integer
          description: Number of tokens the response.
          example: 113
        eval_duration:
          type: integer
          description: Time in nanoseconds spent generating the response.
          format: int64
          example: 1325948000
      description: The response class for the chat endpoint.
    DoneReason:
      anyOf:
        - type: string
        - enum:
            - stop
            - length
            - load
          type: string
      description: Reason why the model is done generating a response.
    Message:
      required:
        - role
        - content
      type: object
      properties:
        role:
          enum:
            - system
            - user
            - assistant
            - tool
          type: string
          description: The role of the message
        content:
          type: string
          description: The content of the message
          example: Why is the sky blue?
        images:
          type: array
          items:
            type: string
            description: Base64-encoded image (for multimodal models such as llava)
            example: iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
          description: (optional) a list of Base64-encoded images to include in the message (for multimodal models such as llava)
        tool_calls:
          type: array
          items:
            $ref: '#/components/schemas/ToolCall'
          description: A list of tools the model wants to call.
      description: A message in the chat endpoint
    Tool:
      type: object
      properties:
        type:
          enum:
            - function
          type: string
          description: The type of tool.
          default: function
        function:
          $ref: '#/components/schemas/ToolFunction'
      description: A tool the model may call.
    ToolFunction:
      required:
        - name
        - description
        - parameters
      type: object
      properties:
        name:
          type: string
          description: The name of the function to be called.
        description:
          type: string
          description: "A description of what the function does, used by the model to choose when and how to call the function.\n"
        parameters:
          $ref: '#/components/schemas/ToolFunctionParams'
      description: A function that the model may call.
    ToolFunctionParams:
      type: object
      description: 'The parameters the functions accepts, described as a JSON Schema object.'
    ToolCall:
      type: object
      properties:
        function:
          $ref: '#/components/schemas/ToolCallFunction'
      description: The tool the model wants to call.
    ToolCallFunction:
      required:
        - name
        - arguments
      type: object
      properties:
        name:
          type: string
          description: The name of the function to be called.
        arguments:
          $ref: '#/components/schemas/ToolCallFunctionArgs'
      description: The function the model wants to call.
    ToolCallFunctionArgs:
      type: object
      description: The arguments to pass to the function.
    GenerateEmbeddingRequest:
      required:
        - model
        - prompt
      type: object
      properties:
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: llama3.2
        prompt:
          type: string
          description: Text to generate embeddings for.
          example: Here is an article about llamas...
        options:
          $ref: '#/components/schemas/RequestOptions'
        keep_alive:
          type: integer
          description: "How long (in minutes) to keep the model loaded in memory.\n\n- If set to a positive duration (e.g. 20), the model will stay loaded for the provided duration.\n- If set to a negative duration (e.g. -1), the model will stay loaded indefinitely.\n- If set to 0, the model will be unloaded immediately once finished.\n- If not set, the model will stay loaded for 5 minutes by default\n"
          nullable: true
      description: Generate embeddings from a model.
    GenerateEmbeddingResponse:
      type: object
      properties:
        embedding:
          type: array
          items:
            type: number
            format: double
          description: The embedding for the prompt.
          example:
            - 0.5670403838157654
            - 0.009260174818336964
            - ...
      description: Returns the embedding information.
    CreateModelRequest:
      required:
        - model
        - modelfile
      type: object
      properties:
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: mario
        modelfile:
          type: string
          description: The contents of the Modelfile.
          example: FROM llama3\nSYSTEM You are mario from Super Mario Bros.
        path:
          type: string
          description: Path to the Modelfile (optional)
        quantize:
          type: string
          description: The quantization level of the model.
          nullable: true
        stream:
          type: boolean
          description: "If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.\n"
          default: true
      description: Create model request object.
    CreateModelResponse:
      type: object
      properties:
        status:
          $ref: '#/components/schemas/CreateModelStatus'
      description: 'Response object for creating a model. When finished, `status` is `success`.'
    CreateModelStatus:
      anyOf:
        - type: string
        - enum:
            - creating system layer
            - parsing modelfile
            - success
          type: string
      description: Status creating the model
    ModelsResponse:
      type: object
      properties:
        models:
          type: array
          items:
            $ref: '#/components/schemas/Model'
          description: List of models available locally.
      description: Response class for the list models endpoint.
    Model:
      type: object
      properties:
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: llama3.2
        modified_at:
          type: string
          description: Model modification date.
          format: date-time
          example: '2023-08-02T17:02:23.7134544-07:00'
        size:
          type: integer
          description: Size of the model on disk.
          format: int64
          example: 7323310500
        digest:
          type: string
          description: The model's digest.
          example: sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711a
        details:
          $ref: '#/components/schemas/ModelDetails'
      description: A model available locally.
    ModelDetails:
      type: object
      properties:
        parent_model:
          type: string
          description: The parent model of the model.
        format:
          type: string
          description: The format of the model.
        family:
          type: string
          description: The family of the model.
        families:
          type: array
          items:
            type: string
          description: The families of the model.
        parameter_size:
          type: string
          description: The size of the model's parameters.
        quantization_level:
          type: string
          description: The quantization level of the model.
      description: Details about a model.
    ModelInformation:
      type: object
      properties:
        general.architecture:
          type: string
          description: The architecture of the model.
        general.file_type:
          type: integer
          description: The file type of the model.
          nullable: true
        general.parameter_count:
          type: integer
          description: The number of parameters in the model.
          format: int64
          nullable: true
        general.quantization_version:
          type: integer
          description: The number of parameters in the model.
          nullable: true
      description: Details about a model.
    ProcessResponse:
      type: object
      properties:
        models:
          type: array
          items:
            $ref: '#/components/schemas/ProcessModel'
          description: List of running models.
      description: Response class for the list running models endpoint.
    ProcessModel:
      type: object
      properties:
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: llama3.2
        size:
          type: integer
          description: Size of the model on disk.
          format: int64
          example: 7323310500
        digest:
          type: string
          description: The model's digest.
          example: sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711a
        details:
          $ref: '#/components/schemas/ModelDetails'
        expires_at:
          type: string
          format: date-time
          example: '2023-08-02T17:02:23.7134544-07:00'
        size_vram:
          type: integer
          description: Size of the model on disk.
          format: int64
          example: 7323310500
      description: A model that is currently loaded.
    ModelInfoRequest:
      required:
        - model
      type: object
      properties:
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: llama3.2
      description: Request class for the show model info endpoint.
    ModelInfo:
      type: object
      properties:
        license:
          type: string
          description: The model's license.
          nullable: true
          example: <contents of license block>
        modelfile:
          type: string
          description: The modelfile associated with the model.
          nullable: true
          example: 'Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llama3:latest\n\nFROM /Users/username/.ollama/models/blobs/sha256:8daa9615cce30c259a9555b1cc250d461d1bc69980a274b44d7eda0be78076d8\nTEMPLATE \"\"\"[INST] {{ if and .First .System }}<<SYS>>{{ .System }}<</SYS>>\n\n{{ end }}{{ .Prompt }} [/INST] \"\"\"\nSYSTEM \"\"\"\"\"\"\nPARAMETER stop [INST]\nPARAMETER stop [/INST]\nPARAMETER stop <<SYS>>\nPARAMETER stop <</SYS>>\n"'
        parameters:
          type: string
          description: The model parameters.
          nullable: true
          example: 'stop [INST]\nstop [/INST]\nstop <<SYS>>\nstop <</SYS>>'
        template:
          type: string
          description: The prompt template for the model.
          nullable: true
          example: '[INST] {{ if and .First .System }}<<SYS>>{{ .System }}<</SYS>>\n\n{{ end }}{{ .Prompt }} [/INST]'
        system:
          type: string
          description: The system prompt for the model.
          nullable: true
        details:
          $ref: '#/components/schemas/ModelDetails'
        model_info:
          $ref: '#/components/schemas/ModelInformation'
        messages:
          type: array
          items:
            $ref: '#/components/schemas/Message'
          description: The default messages for the model.
          nullable: true
      description: 'Details about a model including modelfile, template, parameters, license, and system prompt.'
    CopyModelRequest:
      required:
        - source
        - destination
      type: object
      properties:
        source:
          type: string
          description: Name of the model to copy.
          example: llama3.2
        destination:
          type: string
          description: Name of the new model.
          example: llama3-backup
      description: Request class for copying a model.
    DeleteModelRequest:
      required:
        - model
      type: object
      properties:
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: llama3:13b
      description: Request class for deleting a model.
    PullModelRequest:
      required:
        - model
      type: object
      properties:
        model:
          type: string
          description: "The model name. \n\nModel names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n"
          example: llama3.2
        insecure:
          type: boolean
          description: "Allow insecure connections to the library. \n\nOnly use this if you are pulling from your own library during development.\n"
          default: false
        username:
          type: string
          description: Ollama username.
        password:
          type: string
          description: Ollama password.
        stream:
          type: boolean
          description: "If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.\n"
          default: true
      description: Request class for pulling a model.
    PullModelResponse:
      type: object
      properties:
        status:
          $ref: '#/components/schemas/PullModelStatus'
        digest:
          type: string
          description: The model's digest.
          example: sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711a
        total:
          type: integer
          description: Total size of the model.
          format: int64
          example: 2142590208
        completed:
          type: integer
          description: Total bytes transferred.
          format: int64
          example: 2142590208
      description: "Response class for pulling a model. \n\nThe first object is the manifest. Then there is a series of downloading responses. Until any of the download is completed, the `completed` key may not be included. \n\nThe number of files to be downloaded depends on the number of layers specified in the manifest.\n"
    PullModelStatus:
      anyOf:
        - type: string
        - enum:
            - pulling manifest
            - downloading digestname
            - verifying sha256 digest
            - writing manifest
            - removing any unused layers
            - success
          type: string
      description: Status pulling the model.
      example: pulling manifest
    PushModelRequest:
      required:
        - model
      type: object
      properties:
        model:
          type: string
          description: The name of the model to push in the form of <namespace>/<model>:<tag>.
          example: mattw/pygmalion:latest
        insecure:
          type: boolean
          description: "Allow insecure connections to the library. \n\nOnly use this if you are pushing to your library during development.\n"
          default: false
        username:
          type: string
          description: Ollama username.
        password:
          type: string
          description: Ollama password.
        stream:
          type: boolean
          description: "If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.\n"
          default: true
      description: Request class for pushing a model.
    PushModelResponse:
      type: object
      properties:
        status:
          anyOf:
            - type: string
            - enum:
                - retrieving manifest
                - starting upload
                - pushing manifest
                - success
              type: string
          description: Status pushing the model.
        digest:
          type: string
          description: the model's digest
          example: sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711a
        total:
          type: integer
          description: total size of the model
          format: int64
          example: 2142590208
        completed:
          type: integer
          description: Total bytes transferred.
          format: int64
          example: 2142590208
      description: Response class for pushing a model.
tags:
  - name: Completions
    description: 'Given a prompt, the model will generate a completion.'
  - name: Chat
    description: 'Given a list of messages comprising a conversation, the model will return a response.'
  - name: Embeddings
    description: Get a vector representation of a given input.
  - name: Models
    description: List and describe the various models available.